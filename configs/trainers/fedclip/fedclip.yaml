# configs/trainers/fedclip/fedclip.yaml
TRAINER:
    NAME: "FedCLIPFederated"

MODEL:
    BACKBONE:
      NAME: "ViT-B/32"  # Or ViT-B/16, as you prefer
    DEVICE: "cuda"

OPTIM:
    NAME: "adam"      # You can keep adam, or change to sgd
    LR: 0.001         # Adjust as needed; start with a value, tune later
    WEIGHT_DECAY: 0.0001
    # Add other optimizer settings from the example IF you're using SGD.
    # If you stick with Adam, you don't need the scheduler settings.
    # Example for SGD (if you switch):
    # MAX_EPOCH: 5      # Total epochs (not used directly in federated training)
    # LR_SCHEDULER: "cosine"
    # WARMUP_EPOCH: 1
    # WARMUP_TYPE: "constant"
    # WARMUP_CONS_LR: 1e-4

FED:
  NUM_CLIENTS: 5
  NUM_ROUNDS: 30
  LOCAL_EPOCHS: 10
  EVAL_FREQ: 5

# Add the DATALOADER and INPUT settings from the example:
DATALOADER:
  TRAIN_X:
    BATCH_SIZE: 32  # Use a reasonable batch size. Start with 32, adjust.
  TEST:
    BATCH_SIZE: 100
  NUM_WORKERS: 8    # Number of data loading workers

INPUT:
  SIZE: (224, 224)
  INTERPOLATION: "bicubic"
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]  # CLIP's mean
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]  # CLIP's std
  TRANSFORMS: ["random_resized_crop", "random_flip", "normalize"]

TRAIN:      #This is used by TrainerX
  PRINT_FREQ: 20